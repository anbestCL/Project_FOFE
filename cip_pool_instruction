# copy files from/to server
scp <username>@remote.cip.ifi.lmu.de:<PATH> <LOCAL-FOLDER>
scp <LOCAL-FOLDER> <username>@remote.cip.ifi.lmu.de:<PATH>

# log in on CIP server
# -X to open external windows
ssh -X <username>@remote.cip.ifi.lmu.de

# download files
wget <URL>

# show file content
less <FILE>
# go to end of file
G
# quit view
q   

# edit files on server
nano <FILE>
(^ = Ctrl)

vim <FILE>
i       # insert mode
ESC     # back to command mode
:wq     # command write and quit

# install Python packages
pip3 install --user <PACKAGE>
conda install <PACKAGE>


# Run script on CIP server

# slurm (https://www.rz.ifi.lmu.de/infos/slurm_de.html bzw. https://slurm.schedmd.com/quickstart.html)
# Show groups and names (at least every machine in Antarktis has a suitable GPU)
sinfo

# show running processes
smap/sview(opens an extra window)

# create a new job (will be given a process ID)
# create an executable wrapper script with all necessary parameters (chmod +x <FILE>)
# slurm-<PROCESS_ID>.out will be created (if -o is not used) 
sbatch -p Antarktis -o log.out MY_PROGRAM.sh 

# show your queued jobs
squeue -u <username>

# In case slurm is not working:

# change to specific GPU server
ssh -X <GPU-Rechner>

# check if GPU is availabe
nvcc --version  # GPU version

# check capacity
htop            # list processes
nvidia-smi      # list GPU processes

# run program in background (remember the server name!)
# remember to redirect all outputs to log file (you can't scroll in screen)

# CAUTION: Servers are restarted every sunday night, all running processes will be killed 
# --> Use checkpoints in your scripts!
screen
# detatch from screen
Strg+A D
# list active screens
screen -list
# resume screen
screen -r <ID>


# GPU with Pytorch

# get installation line with GPU settings from https://pytorch.org/

# check GPU in python:
python3
import torch
torch.cuda.is_available()
# True
torch.has_cudnn
# True

# use GPU (small changes in code necessary)
.cuda() # move model/data to GPU
.cpu()  # move back to CPU (if necessary)